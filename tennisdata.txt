1.Decision tree
import pandas as pd
import numpy as np

# Define a Node class to represent each node in the decision tree
class Node:
    def _init_(self, feature=None, threshold=None, left=None, right=None, *, value=None):
        self.feature = feature           # Index of the feature to split on
        self.threshold = threshold       # Threshold value for the feature
        self.left = left                 # Left child node
        self.right = right               # Right child node
        self.value = value               # Value if the node is a leaf

    def is_leaf_node(self):
        return self.value is not None

# Define the DecisionTreeClassifier class
class DecisionTreeClassifier:
    def _init_(self, max_depth=100):
        self.max_depth = max_depth
        self.root = None

    def fit(self, X, y):
        self.root = self._grow_tree(X, y)

    def predict(self, X):
        return np.array([self._traverse_tree(x, self.root) for x in X])

    def _grow_tree(self, X, y, depth=0):
        n_samples, n_features = X.shape
        n_labels = len(np.unique(y))

        # Stopping criteria
        if depth >= self.max_depth or n_labels == 1 or n_samples < 2:
            leaf_value = self._most_common_label(y)
            return Node(value=leaf_value)

        rnd_feats = np.random.choice(n_features, n_features, replace=False)

        # Find the best split
        best_feat, best_thresh = self._best_criteria(X, y, rnd_feats)
        
        # Grow the children recursively
        left_idx, right_idx = self._split(X[:, best_feat], best_thresh)
        left = self._grow_tree(X[left_idx, :], y[left_idx], depth + 1)
        right = self._grow_tree(X[right_idx, :], y[right_idx], depth + 1)
        return Node(best_feat, best_thresh, left, right)

    def _best_criteria(self, X, y, features):
        best_gain = -1
        split_idx, split_thresh = None, None
        for feat in features:
            X_column = X[:, feat]
            thresholds = np.unique(X_column)
            for threshold in thresholds:
                gain = self._information_gain(y, X_column, threshold)
                if gain > best_gain:
                    best_gain = gain
                    split_idx = feat
                    split_thresh = threshold
        return split_idx, split_thresh

    def _information_gain(self, y, X_column, split_thresh):
        # Parent Gini impurity
        parent_gini = self._gini(y)

        # Generate split
        left_idx, right_idx = self._split(X_column, split_thresh)

        if len(left_idx) == 0 or len(right_idx) == 0:
            return 0

        # Weighted avg Gini of children
        n = len(y)
        n_l, n_r = len(left_idx), len(right_idx)
        e_l, e_r = self._gini(y[left_idx]), self._gini(y[right_idx])
        child_gini = (n_l / n) * e_l + (n_r / n) * e_r

        # Information gain is parent impurity minus child impurity
        ig = parent_gini - child_gini
        return ig

    def _gini(self, y):
        m = len(y)
        return 1.0 - sum((np.sum(y == c) / m) ** 2 for c in np.unique(y))

    def _split(self, X_column, split_thresh):
        left_idx = np.argwhere(X_column <= split_thresh).flatten()
        right_idx = np.argwhere(X_column > split_thresh).flatten()
        return left_idx, right_idx

    def _traverse_tree(self, x, node):
        if node.is_leaf_node():
            return node.value
        if x[node.feature] <= node.threshold:
            return self._traverse_tree(x, node.left)
        return self._traverse_tree(x, node.right)

    def _most_common_label(self, y):
        return np.bincount(y).argmax()

# Step 1: Create the CSV file
data = {
    'Day': ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14'],
    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],
    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],
    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],
    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],
    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
}

df = pd.DataFrame(data)
csv_file_path = 'tennis.csv'
df.to_csv(csv_file_path, index=False)

# Step 2: Load the CSV file and apply the Decision Tree Algorithm
# Load dataset
df = pd.read_csv(csv_file_path)

# Convert categorical variables to numeric
df['Outlook'] = df['Outlook'].map({'Sunny': 0, 'Overcast': 1, 'Rain': 2})
df['Temperature'] = df['Temperature'].map({'Hot': 0, 'Mild': 1, 'Cool': 2})
df['Humidity'] = df['Humidity'].map({'High': 0, 'Normal': 1})
df['Wind'] = df['Wind'].map({'Weak': 0, 'Strong': 1})
df['PlayTennis'] = df['PlayTennis'].map({'No': 0, 'Yes': 1})

# Split the data into features and target
X = df.drop(['Day', 'PlayTennis'], axis=1).values
y = df['PlayTennis'].values

# Initialize and train the Decision Tree Classifier
clf = DecisionTreeClassifier(max_depth=3)
clf.fit(X, y)

# Make predictions
y_pred = clf.predict(X)

# Evaluate the model
accuracy = np.sum(y == y_pred) / len(y)
print(f"Accuracy: {accuracy}")

2.K means
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Step 1: Create the CSV file
data = {
    'Individual': [1, 2, 3, 4, 5, 6, 7],
    'Variable 1': [1.0, 1.5, 3.0, 5.0, 3.5, 4.5, 3.5],
    'Variable 2': [1.0, 2.0, 4.0, 7.0, 5.0, 5.0, 4.5]
}

df = pd.DataFrame(data)
csv_file_path = 'kmeans_data.csv'
df.to_csv(csv_file_path, index=False)

# Step 2: Implement K-means Clustering Algorithm from Scratch
class KMeans:
    def _init_(self, K=2, max_iters=100):
        self.K = K
        self.max_iters = max_iters
        self.centroids = None

    def fit(self, X):
        # Randomly initialize centroids
        self.centroids = X[np.random.choice(X.shape[0], self.K, replace=False)]
        for _ in range(self.max_iters):
            # Assign clusters
            clusters = self._assign_clusters(X)
            # Calculate new centroids
            new_centroids = self._calculate_centroids(X, clusters)
            # Check for convergence
            if np.all(self.centroids == new_centroids):
                break
            self.centroids = new_centroids

    def predict(self, X):
        return self._assign_clusters(X)

    def _assign_clusters(self, X):
        distances = np.array([[np.linalg.norm(x - centroid) for centroid in self.centroids] for x in X])
        return np.argmin(distances, axis=1)

    def _calculate_centroids(self, X, clusters):
        return np.array([X[clusters == k].mean(axis=0) for k in range(self.K)])

# Load the CSV file
df = pd.read_csv(csv_file_path)

# Extract features
X = df[['Variable 1', 'Variable 2']].values

# Initialize and fit the KMeans model
kmeans = KMeans(K=2, max_iters=100)
kmeans.fit(X)

# Predict the clusters
clusters = kmeans.predict(X)

# Add the cluster labels to the dataframe
df['Cluster'] = clusters

# Print the resulting clusters
print(df)

# Step 3: Visualize the clusters
def plot_clusters(df, kmeans):
    plt.figure(figsize=(8, 6))
    
    # Plot each cluster
    for cluster in range(kmeans.K):
        cluster_points = df[df['Cluster'] == cluster]
        plt.scatter(cluster_points['Variable 1'], cluster_points['Variable 2'], label=f'Cluster {cluster}')
    
    # Plot centroids
    for centroid in kmeans.centroids:
        plt.scatter(*centroid, s=200, marker='X', c='black')
    
    plt.xlabel('Variable 1')
    plt.ylabel('Variable 2')
    plt.title('K-means Clustering')
    plt.legend()
    plt.show()

plot_clusters(df, kmeans)

3.linear regression
import numpy as np

# Given data points
x = np.array([0, 1, 2, 3, 4])
y = np.array([2, 3, 5, 4, 6])

# a) Find the linear regression line y = ax + b
# Calculate the means of x and y
x_mean = np.mean(x)
y_mean = np.mean(y)

# Calculate the terms needed for the numerator and denominator of 'a'
numerator = 0
denominator = 0
for i in range(len(x)):
    numerator += (x[i] - x_mean) * (y[i] - y_mean)
    denominator += (x[i] - x_mean) ** 2

# Calculate 'a' and 'b'
a = numerator / denominator
b = y_mean - a * x_mean

print(f"Linear regression line: y = {a:.2f}x + {b:.2f}")

# b) Estimate the value of y when x = 10
x_new = 10
y_new = a * x_new + b
print(f"Estimated value of y when x = 10: {y_new:.2f}")

# c) Calculate the error
# Error is the sum of squared differences between actual and predicted values
error = 0
for i in range(len(x)):
    y_pred = a * x[i] + b
    error += (y[i] - y_pred) ** 2

print(f"Sum of squared errors: {error:.2f}")

# Additionally, to calculate mean squared error (MSE) for better error representation
mse = error / len(x)
print(f"Mean squared error: {mse:.2f}")

4.Clustering

import numpy as np
import matplotlib.pyplot as plt

# Given points
points = np.array([
    [2, 10],  # A1
    [2, 5],   # A2
    [8, 4],   # A3
    [5, 8],   # A4
    [7, 5],   # A5
    [6, 4],   # A6
    [1, 2],   # A7
    [4, 9]    # A8
])

# Initial cluster centers
initial_centers = np.array([
    [2, 10],  # Center for cluster 1 (A1)
    [5, 8],   # Center for cluster 2 (A4)
    [1, 2]    # Center for cluster 3 (A7)
])

# Function to compute the Euclidean distance
def euclidean_distance(a, b):
    return np.sqrt(np.sum((a - b) ** 2))

# Function to assign points to the nearest cluster center
def assign_clusters(points, centers):
    clusters = {}
    for i in range(len(centers)):
        clusters[i] = []
    for point in points:
        distances = [euclidean_distance(point, center) for center in centers]
        cluster = np.argmin(distances)
        clusters[cluster].append(point)
    return clusters

# Function to recalculate the cluster centers
def recalculate_centers(clusters):
    new_centers = []
    for cluster in clusters.values():
        new_center = np.mean(cluster, axis=0)
        new_centers.append(new_center)
    return new_centers

# K-means clustering algorithm
def kmeans(points, initial_centers, max_iterations=100):
    centers = initial_centers
    for _ in range(max_iterations):
        clusters = assign_clusters(points, centers)
        new_centers = recalculate_centers(clusters)
        if np.allclose(centers, new_centers):
            break
        centers = new_centers
    return centers, clusters

# Run the K-means algorithm
final_centers, clusters = kmeans(points, initial_centers)

print("Final cluster centers:")
for i, center in enumerate(final_centers):
    print(f"Cluster {i+1} center: {center}")

print("\nCluster assignments:")
for i, cluster in clusters.items():
    print(f"Cluster {i+1}: {cluster}")

# Convert final_centers to a numpy array
final_centers = np.array(final_centers)

# Plot the results
colors = ['r', 'b', 'g']
labels = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8']
for i, cluster in clusters.items():
    cluster_points = np.array(cluster)
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], color=colors[i], label=f'Cluster {i+1}')
    for point in cluster_points:
        plt.text(point[0], point[1], labels[np.where((points == point).all(axis=1))[0][0]], fontsize=12)
plt.scatter(final_centers[:, 0], final_centers[:, 1], color='k', marker='x', s=100, label='Centers')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title('K-means Clustering')
plt.show()


5.statistical measures
import re
from datetime import datetime

# Dataset
data = [
    [1, 21, "1L", "Male", "31.05.1992", "No"],
    [2, 35, "1,00,000", "Male", "10-05-2002", "Yes"],
    [3, 26, "45000", "Male", "Aug 5, 2000", "Yes"],
    [4, 45, "", "Male", " ", "No"],
    [5, 67, "10,000", "Female", "31.03.1986", "Yes"],
    [6, 32, "10000", "Female", "10/5/1987", "Yes"],
    [7, 32, "5$", "Female", "31.05.1992", "Yes"],
    [8, 31, "5 Dollars", "Male", "10-05-2002", "No"],
    [9, 10, "10,000", "Female", "Aug 5, 2000", "Yes"],
    [10, 42, "15000", "Female", "Sep 12'2000", "Yes"],
    [11, "", "25,000", "Female", "31.03.1986", "Yes"],
    [12, 32, "35000", "Male", "10/5/1987", "No"],
    [13, 35, "150000", "Female", "Sep 12'2000", "Yes"],
    [14, 35, "35000", "Male", "31.03.1986", "No"]
]

# Function to clean income
def clean_income(income):
    if income == "":
        return 0
    income = re.sub(r'[^\d]', '', income)
    return int(income)

# Function to clean DoB
def clean_dob(dob):
    if dob.strip() == "":
        return None
    dob = re.sub(r'[^\w\s]', ' ', dob)
    try:
        return datetime.strptime(dob, "%d %m %Y").strftime("%Y-%m-%d")
    except ValueError:
        return None

# Clean data
for row in data:
    row[2] = clean_income(row[2])
    row[4] = clean_dob(row[4])
    if row[1] == "":
        row[1] = 0  # Assuming 0 for missing age

# Helper functions for statistical measures
def mean(values):
    total = sum(values)
    count = len(values)
    return total / count if count != 0 else 0

def median(values):
    values = sorted(values)
    n = len(values)
    mid = n // 2
    if n % 2 == 0:
        return (values[mid - 1] + values[mid]) / 2
    else:
        return values[mid]

def mode(values):
    frequency = {}
    for value in values:
        frequency[value] = frequency.get(value, 0) + 1
    max_freq = max(frequency.values())
    modes = [key for key, value in frequency.items() if value == max_freq]
    return modes

def standard_deviation(values):
    avg = mean(values)
    variance = mean([(x - avg) ** 2 for x in values])
    return variance ** 0.5

def gender_count(data):
    count = {'Male': 0, 'Female': 0}
    for row in data:
        count[row[3]] += 1
    return count

# Extract columns for statistical measures
ages = [row[1] for row in data if row[1] != 0]
incomes = [row[2] for row in data if row[2] != 0]

# Calculate statistical measures
mean_age = mean(ages)
median_age = median(ages)
mode_income = mode(incomes)
std_dev_income = standard_deviation(incomes)
gender_distribution = gender_count(data)

# Print results
print(f"Mean Age: {mean_age}")
print(f"Median Age: {median_age}")
print(f"Mode of Income: {mode_income}")
print(f"Standard Deviation of Income: {std_dev_income}")
print(f"Gender Count: {gender_distribution}")

# Print cleaned data
print("\nCleaned Data:")
for row in data:
    print(row)

6.K means
import numpy as np
import matplotlib.pyplot as plt

# Initial centers
centers = np.array([[2, 2], [1, 1]])

# Points
points = np.array([[2, 2], [3, 2], [1, 1], [3, 1], [1.5, 0.5]])

# Function to compute the distance between two points
def distance(a, b):
    return np.sqrt(np.sum((a - b) ** 2))

# K-Means algorithm
def k_means(points, centers):
    # Assign points to the nearest center
    clusters = {i: [] for i in range(len(centers))}
    for point in points:
        distances = [distance(point, center) for center in centers]
        closest_center = np.argmin(distances)
        clusters[closest_center].append(point)

    # Update the centers
    new_centers = np.array([np.mean(clusters[i], axis=0) for i in range(len(centers))])
    
    return clusters, new_centers

# Plotting function
def plot_clusters(clusters, centers):
    colors = ['r', 'g', 'b', 'c', 'm']
    for idx, cluster in clusters.items():
        cluster = np.array(cluster)
        plt.scatter(cluster[:, 0], cluster[:, 1], c=colors[idx], label=f'Cluster {idx+1}')
    plt.scatter(centers[:, 0], centers[:, 1], c='black', marker='x', label='Centers')
    plt.xlabel('X-axis')
    plt.ylabel('Y-axis')
    plt.legend()
    plt.title('K-Means Clustering')
    plt.show()

# Perform K-Means clustering
clusters, new_centers = k_means(points, centers)

# Plot initial clusters
plot_clusters(clusters, new_centers)

# Repeat until centers do not change
iteration = 1
while not np.array_equal(centers, new_centers):
    print(f'Iteration {iteration}: Centers updated to {new_centers}')
    centers = new_centers
    clusters, new_centers = k_means(points, centers)
    iteration += 1

# Final plot
print(f'Final centers: {new_centers}')
plot_clusters(clusters, new_centers)

7.Apriori algo
# Dataset
transactions = {
    'T1': {'HotDogs', 'Buns', 'Ketchup'},
    'T2': {'HotDogs', 'Buns'},
    'T3': {'HotDogs', 'Coke', 'Chips'},
    'T4': {'Chips', 'Coke'},
    'T5': {'Chips', 'Ketchup'},
    'T6': {'HotDogs', 'Coke', 'Chips'}
}

# Parameters
support_threshold = 33.34 / 100
confidence_threshold = 60 / 100

# Function to calculate support
def calculate_support(itemset, transactions):
    count = 0
    for transaction in transactions.values():
        if itemset.issubset(transaction):
            count += 1
    return count / len(transactions)

# Function to generate combinations of a certain length
def generate_combinations(items, length):
    if length == 0:
        return [set()]
    elif length == 1:
        return [{item} for item in items]
    else:
        combinations = []
        for i in range(len(items)):
            for subset in generate_combinations(items[i+1:], length-1):
                combinations.append({items[i]} | subset)
        return combinations

# Apriori algorithm
def apriori(transactions, support_threshold):
    items = set(item for transaction in transactions.values() for item in transaction)
    candidates = [frozenset([item]) for item in items]
    frequent_itemsets = []
    k = 1
    while candidates:
        print(f"Scanning for itemsets of length {k}")
        candidate_supports = {item: calculate_support(item, transactions) for item in candidates}
        frequent_items = {item for item, support in candidate_supports.items() if support >= support_threshold}
        frequent_itemsets.extend(frequent_items)
        print(f"Candidates: {candidates}")
        print(f"Frequent itemsets: {frequent_items}")
        k += 1
        candidates = generate_combinations(list(set(item for itemset in frequent_items for item in itemset)), k)
        candidates = [frozenset(candidate) for candidate in candidates]
    return frequent_itemsets

# Generate association rules
def generate_rules(frequent_itemsets, transactions, confidence_threshold):
    rules = []
    for itemset in frequent_itemsets:
        itemset_list = list(itemset)
        for i in range(1, len(itemset_list)):
            for antecedent in generate_combinations(itemset_list, i):
                antecedent = frozenset(antecedent)
                consequent = itemset - antecedent
                support = calculate_support(itemset, transactions)
                confidence = support / calculate_support(antecedent, transactions)
                if confidence >= confidence_threshold:
                    rules.append((antecedent, consequent, confidence))
    return rules

# Run Apriori algorithm
frequent_itemsets = apriori(transactions, support_threshold)
print("\nAll Frequent Itemsets:")
for itemset in frequent_itemsets:
    print(itemset)

# Generate and sort rules by confidence
rules = generate_rules(frequent_itemsets, transactions, confidence_threshold)
rules.sort(key=lambda x: x[2], reverse=True)

print("\nStrong Association Rules (sorted by confidence):")
for antecedent, consequent, confidence in rules:
    print(f"{set(antecedent)} -> {set(consequent)} (confidence: {confidence:.2f})")
